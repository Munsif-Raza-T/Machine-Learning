{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442bb264",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using RNN(Recurrent Neural Network)s\n",
    "\n",
    "## Munsif Raza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e4890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325227b",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "We shall use IMDB dataset. It consists of 50,000 movie reviews, of which 25,000 are tutorials and 25,000 tests. Half of these comments were labeled as positive and the other half as negative. It is a balanced dataset as the positive and negative labels in this dataset are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ce1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "VOCAB_SIZE = 88584\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64\n",
    "(train_data, train_labels),(test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7bdc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]))\n",
    "print(len(train_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e2d54a",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "You see our reviews contain different length of words which we can't give to a neural network. we have to make every review of same size for that we can do two things.\n",
    "1. If review has more than 250 words trim it\n",
    "2. If review has less than 250 words add remaining as 0's\n",
    "\n",
    "by doing so we shall make every review to come on same platform regarding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00fb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca53159",
   "metadata": {},
   "source": [
    "# Creating model\n",
    "We shall use a word embedding layer as the first layer in our model and add a LSTM layer afterwards that feeds into a dense node to get our predicted sentiment.\n",
    "Here 32 stands for the output dimension of the vectors generated by the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ab2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a95bb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c295e3",
   "metadata": {},
   "source": [
    "# Training the model.\n",
    "Now that we have got a model let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a1aec93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 36s 55ms/step - loss: 0.4409 - acc: 0.7967 - val_loss: 0.4461 - val_acc: 0.8290\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 35s 55ms/step - loss: 0.2374 - acc: 0.9103 - val_loss: 0.2721 - val_acc: 0.8894\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 35s 57ms/step - loss: 0.1827 - acc: 0.9327 - val_loss: 0.2798 - val_acc: 0.8910\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 35s 55ms/step - loss: 0.1500 - acc: 0.9469 - val_loss: 0.3035 - val_acc: 0.8966\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.1273 - acc: 0.9546 - val_loss: 0.3003 - val_acc: 0.8924\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['acc'])\n",
    "history = model.fit(train_data, train_labels, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "705a2fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 8s 11ms/step - loss: 0.3917 - acc: 0.8586\n",
      "[0.3916893005371094, 0.8586400151252747]\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate the model by using testing data.\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b73d0b",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "\n",
    "Let's use our model to make prediction on our own reviews.\n",
    "Since our reviews are encoded. We'll need to convert any review that we write into that form so the model can understandit.\n",
    "To do that We'll load the encodings form the dataset and use them to encode our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "382d687c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2648b3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "# Let's make a decode function\n",
    "\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:\n",
    "            text += reverse_word_index[num]+\" \"\n",
    "    \n",
    "    return text[:-1]\n",
    "\n",
    "print(decode_integers(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "512ccd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is Positive Review\n",
      "It is Negative Review\n"
     ]
    }
   ],
   "source": [
    "# Now let's make predictions\n",
    "\n",
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1,250))\n",
    "    pred[0] = encoded_text\n",
    "    result = model.predict(pred)\n",
    "    if result[0] > 0.5:\n",
    "        print('It is Positive Review')\n",
    "    else:\n",
    "        print('It is Negative Review')\n",
    "    \n",
    "positive_review = \"That movie was I just loved it and definately will watch again\"\n",
    "predict(positive_review)\n",
    "\n",
    "negative_review = \"That movie was bad, I hate that one of the worst movies it is, it was waste of time. it really sucked I would not watch it again\"\n",
    "predict(negative_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df79075",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We used RNNs to train the model with categorical data and then used our own data to predict the review type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
